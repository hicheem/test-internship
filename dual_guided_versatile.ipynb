{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvgeZ7WZQCig"
      },
      "outputs": [],
      "source": [
        "# Install the required libraries\n",
        "!pip install diffusers[\"torch\"] transformers datasets einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1cr8hOq-o-k5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam, AdamW\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from accelerate import Accelerator\n",
        "from datasets import load_dataset\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from models import UNetModelVD_DualGuided\n",
        "\n",
        "from transformers import CLIPTextModelWithProjection, CLIPTokenizer, CLIPVisionModelWithProjection, CLIPImageProcessor\n",
        "from diffusers import AutoencoderKL, DDIMScheduler\n",
        "from diffusers.image_processor import VaeImageProcessor\n",
        "\n",
        "from diffusers.utils.torch_utils import randn_tensor\n",
        "\n",
        "# Set the default torch datatype to float16 to optimize memory usage and avoid RAM issues\n",
        "torch.set_default_dtype(torch.float16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03wgP6GQJPtR"
      },
      "outputs": [],
      "source": [
        "# Download the pretrained weights of the Versatile Diffusion model and rename them from 'diffuser.text/image' to 'unet_text/image'\n",
        "# In the official implementation, 'diffuser.text/image' represents the model. In this context, we are using 'unet_text/image'\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "vd_path = hf_hub_download('shi-labs/versatile-diffusion-model', 'pretrained_pth/vd-four-flow-v1-0-fp16.pth')\n",
        "\n",
        "state_dict = torch.load(vd_path)\n",
        "\n",
        "renamed_state_dict = {}\n",
        "\n",
        "for key, value in state_dict.items():\n",
        "    if key.startswith(\"diffuser.text.\"):\n",
        "        new_key = key.replace(\"diffuser.text.\", \"unet_text.\")\n",
        "    elif key.startswith(\"diffuser.image.\"):\n",
        "        new_key = key.replace(\"diffuser.image.\", \"unet_image.\")\n",
        "    else:\n",
        "        new_key = key\n",
        "    renamed_state_dict[new_key] = value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zm9yKQXPwna6"
      },
      "outputs": [],
      "source": [
        "# Download a minimal dataset consisting of image-text pairs for experimentation.\n",
        "dataset = load_dataset(\"lambdalabs/pokemon-blip-captions\")\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, vae_encoder_img2latent, image_context_encode, text_context_encode, transform):\n",
        "    self.dataset = dataset['train']\n",
        "    self.vae_encoder_img2latent = vae_encoder_img2latent\n",
        "    self.image_context_encode = image_context_encode\n",
        "    self.text_context_encode = text_context_encode\n",
        "    self.transform = transform\n",
        "    self.device = \"cuda\"\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    data = self.dataset[idx]\n",
        "\n",
        "    # Get image\n",
        "    image = self.transform(data['image'].convert(\"RGB\"))\n",
        "\n",
        "    # Process image through VAE encoder\n",
        "    image_latent = self.vae_encoder_img2latent(image.to(self.device).half()).squeeze(0)\n",
        "\n",
        "    # Process image through CLIP image encoder\n",
        "    image_context = self.image_context_encode(data['image']).squeeze(0)\n",
        "    \n",
        "    # Get the corresponding text\n",
        "    text = data['text']\n",
        "\n",
        "    # Process text through CLIP text encoder\n",
        "    text_context = self.text_context_encode(text).squeeze(0)\n",
        "\n",
        "    return image_latent, image_context, text_context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYEybvY4OfeU"
      },
      "outputs": [],
      "source": [
        "def load_config(model, sub_folder):\n",
        "    return model.from_pretrained(\n",
        "      \"shi-labs/versatile-diffusion\", subfolder=sub_folder)\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "# Load the configuration for various components: VAE, CLIP, and the final model.\n",
        "# .half() will cast to float16 to optimize memory usage\n",
        "def load_models():\n",
        "  scheduler = load_config(DDIMScheduler, \"scheduler\")\n",
        "  with torch.no_grad():\n",
        "    vae = load_config(AutoencoderKL, \"vae\").half().to(device).requires_grad_(False)\n",
        "    tokenizer = load_config(CLIPTokenizer, \"tokenizer\")\n",
        "    text_encoder = load_config(CLIPTextModelWithProjection, \"text_encoder\").half().to(device).requires_grad_(False)\n",
        "    image_feature_extractor = load_config(CLIPImageProcessor, \"image_feature_extractor\")\n",
        "    image_encoder = load_config(CLIPVisionModelWithProjection, \"image_encoder\").half().to(device).requires_grad_(False)\n",
        "  # Instantiate the Dual Guided model.\n",
        "  model = UNetModelVD_DualGuided().half().to(device)\n",
        "  # Load the pretrained model weights.\n",
        "  model.load_state_dict(renamed_state_dict, strict=False)\n",
        "  return model, scheduler, vae, tokenizer, text_encoder, image_feature_extractor, image_encoder\n",
        "\n",
        "model, scheduler, vae, tokenizer, text_encoder, image_feature_extractor, image_encoder = load_models()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "PiYVKMD54OuX"
      },
      "outputs": [],
      "source": [
        "class DualGuidedVersatileDiffusion():\n",
        "\n",
        "  def __init__(self):\n",
        "    super(DualGuidedVersatileDiffusion, self).__init__()\n",
        "\n",
        "    self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    self.model = model\n",
        "    self.scheduler = scheduler\n",
        "    self.vae = vae\n",
        "    self.tokenizer = tokenizer\n",
        "    self.text_encoder = text_encoder\n",
        "    self.image_feature_extractor = image_feature_extractor\n",
        "    self.image_encoder = image_encoder\n",
        "\n",
        "    self.vae_scale_factor = 2 ** (len(self.vae.config.block_out_channels) - 1)\n",
        "    self.image_processor = VaeImageProcessor(vae_scale_factor=self.vae_scale_factor)\n",
        "\n",
        "    self.optimizer = AdamW(self.model.parameters(), lr=1e-4)\n",
        "    self.mse_loss = nn.MSELoss()\n",
        "    self.batch_size = 1\n",
        "    self.height = 256\n",
        "    self.width = 256\n",
        "\n",
        "    self.transform = transforms.Compose(\n",
        "        [\n",
        "          transforms.Resize((self.height, self.width)),\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize([0.5], [0.5]),\n",
        "        ]\n",
        "      )\n",
        "    self.data_loader = DataLoader(\n",
        "        CustomDataset(self.vae_encoder_img2latent, self.image_context_encode, self.text_context_encode, self.transform),\n",
        "        batch_size=self.batch_size, shuffle=True\n",
        "    )\n",
        "\n",
        "\n",
        "  # Encode and normalize text context\n",
        "  def text_context_encode(self, text):\n",
        "    text_inputs = self.tokenizer(\n",
        "          text,\n",
        "          padding=\"max_length\",\n",
        "          max_length= self.tokenizer.model_max_length,\n",
        "          truncation=True,\n",
        "          return_tensors=\"pt\",\n",
        "          )\n",
        "    text_embeds = self.text_encoder(text_inputs.input_ids.to(self.device))\n",
        "    embeds = self.text_encoder.text_projection(text_embeds.last_hidden_state)\n",
        "    embeds_pooled = text_embeds.text_embeds\n",
        "    embeds = embeds / torch.norm(embeds_pooled.unsqueeze(1), dim=-1, keepdim=True)\n",
        "    return embeds\n",
        "\n",
        "  # Encode and normalize image context\n",
        "  def image_context_encode(self, image):\n",
        "    image_features = self.image_feature_extractor(images=image, return_tensors=\"pt\")\n",
        "    image_features = image_features.pixel_values.to(self.device).to(self.image_encoder.dtype)\n",
        "    image_embeds = self.image_encoder(image_features)\n",
        "    embeds = self.image_encoder.vision_model.post_layernorm(image_embeds.last_hidden_state)\n",
        "    embeds = self.image_encoder.visual_projection(embeds)\n",
        "    embeds_pooled = embeds[:, 0:1]\n",
        "    embeds = embeds / torch.norm(embeds_pooled, dim=-1, keepdim=True)\n",
        "    return embeds\n",
        "\n",
        "  # Encode input image into the latent space\n",
        "  def vae_encoder_img2latent(self, image):\n",
        "    image = image.unsqueeze(0)\n",
        "    encoded = self.vae.encode(image)\n",
        "    latent = encoded.latent_dist.sample() * self.vae.config.scaling_factor\n",
        "    return latent\n",
        "\n",
        "  # Training\n",
        "  #========================================================================================\n",
        "  def train_step(self, mixing_type=\"attention\"):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    train_bar = tqdm(self.data_loader, desc='Training')\n",
        "    self.model.train()\n",
        "\n",
        "    for step, batch in enumerate(train_bar):\n",
        "\n",
        "      self.optimizer.zero_grad()\n",
        "\n",
        "      image_latents, image_context, text_context = batch\n",
        "\n",
        "      # Generate sample noise to be added to the image\n",
        "      noise = torch.randn(image_latents.shape).to(image_latents.device)\n",
        "\n",
        "      # Get a random timestep\n",
        "      timesteps = torch.randint(0, self.scheduler.config.num_train_timesteps, (self.batch_size,), device=image_latents.device).long()\n",
        "\n",
        "      # Add noise at the specified timestep\n",
        "      noisy_latents = self.scheduler.add_noise(image_latents, noise, timesteps).half().to(self.device)\n",
        "\n",
        "      # For Classifier-free guidance\n",
        "      unconditional_image_context = self.image_context_encode([np.zeros((self.height, self.width, 3)) + 0.5] * 1)\n",
        "      unconditional_text_context = self.text_context_encode([\"\"] * 1)\n",
        "\n",
        "      # Train the model 10% unconditionally and 90% conditionally\n",
        "      # In this case, we consider image input and image context the same (it would be better if we have a tailored dataset)\n",
        "      if torch.rand(1).item() < 0.1:\n",
        "        noise_pred = self.model.forward(noisy_latents, timesteps, xtype=\"image\", c0=unconditional_image_context, c1=unconditional_text_context, c0_type='image', c1_type='text', c0_ratio=0.4, c1_ratio=0.6, mixing_type=mixing_type)\n",
        "      else:\n",
        "        noise_pred = self.model.forward(noisy_latents, timesteps, xtype=\"image\", c0=image_context, c1=text_context, c0_type='image', c1_type='text', c0_ratio=0.4, c1_ratio=0.6, mixing_type=mixing_type)\n",
        "\n",
        "      loss = self.mse_loss(noise_pred.float(), noise.float())\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      self.optimizer.step()\n",
        "\n",
        "      running_loss += loss.item()\n",
        "\n",
        "      train_bar.set_postfix({'mse': f'{running_loss / (step+1):.4f}'})\n",
        "\n",
        "    total_loss = running_loss / (len(self.data_loader) * self.batch_size)\n",
        "    return total_loss\n",
        "\n",
        "  def train(self, epochs):\n",
        "\n",
        "    for epoch in epochs:\n",
        "\n",
        "      print(\"Epoch : \", epoch, \" Start\")\n",
        "      loss = self.train_step()\n",
        "      print(\"Epoch : \", epoch, \" End with loss = \", loss)\n",
        "\n",
        "  #========================================================================================\n",
        "  \n",
        "  # Sampling\n",
        "\n",
        "  def prepare_context_sampling(self, image_context, text_context):\n",
        "    conditional_image_context = self.image_context_encode(image_context)\n",
        "    conditional_text_context = self.text_context_encode(text_context)\n",
        "\n",
        "    unconditional_image_context = self.image_context_encode([np.zeros((self.height, self.width, 3)) + 0.5] * 1)\n",
        "    unconditional_text_context = self.text_context_encode([\"\"] * 1)\n",
        "\n",
        "    image_embeddings = torch.cat([unconditional_image_context, conditional_image_context], axis=0)\n",
        "    text_embeddings = torch.cat([unconditional_text_context, conditional_text_context], axis=0)\n",
        "\n",
        "    return image_embeddings, text_embeddings\n",
        "\n",
        "  # Generate a sample noise \n",
        "  def prepare_latents_sampling(self, num_channels_latents, device, dtype, batch_size=1, latents=None):\n",
        "    shape = (batch_size, num_channels_latents, self.height // self.vae_scale_factor, self.width // self.vae_scale_factor)\n",
        "    generator = torch.Generator(device=self.vae.device).manual_seed(0)\n",
        "    if isinstance(generator, list) and len(generator) != batch_size:\n",
        "        raise ValueError(\n",
        "            f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n",
        "            f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n",
        "        )\n",
        "\n",
        "    if latents is None:\n",
        "        latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n",
        "    else:\n",
        "        latents = latents.to(device)\n",
        "\n",
        "    # scale the initial noise by the standard deviation required by the scheduler\n",
        "    latents = latents * self.scheduler.init_noise_sigma\n",
        "    return latents\n",
        "\n",
        "  # Decode the generated latent image\n",
        "  def decode_latents_sampling(self, latents):\n",
        "\n",
        "    latents = 1 / self.vae.config.scaling_factor * latents\n",
        "    image = self.vae.decode(latents, return_dict=False)[0]\n",
        "    image = (image / 2 + 0.5).clamp(0, 1)\n",
        "    image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n",
        "    return image\n",
        "\n",
        "  def sampling(self, image, text, guidance_factor=7.5):\n",
        "    self.scheduler.set_timesteps(1000)\n",
        "    latents = self.prepare_latents_sampling(4,  self.vae.device, torch.float16,)\n",
        "    image_embeddings, text_embeddings = self.prepare_context_sampling(image, text)\n",
        "    for t in tqdm(self.scheduler.timesteps):\n",
        "      # Expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
        "      latent_model_input = torch.cat([latents] * 2)\n",
        "      t = torch.tensor([t-1])\n",
        "      t = torch.cat([t] * 2).to(device)\n",
        "\n",
        "      latent_model_input = self.scheduler.scale_model_input(latent_model_input, timestep=t)\n",
        "\n",
        "      # Predict the noise residual\n",
        "      with torch.no_grad():\n",
        "        noise_pred = self.model.forward(latent_model_input, t, xtype=\"image\", c0=image_embeddings, c1=text_embeddings, c0_type='image', c1_type='text', c0_ratio=0.4, c1_ratio=0.6, mixing_type=\"attention\")\n",
        "\n",
        "      # Perform guidance\n",
        "      noise_pred_uncond, noise_pred_cond = noise_pred.chunk(2)\n",
        "      # Linear interpolation\n",
        "      noise_pred = noise_pred_uncond + guidance_factor * (noise_pred_cond - noise_pred_uncond)\n",
        "\n",
        "      # compute the previous noisy sample x_t -> x_t-1\n",
        "      latents = self.scheduler.step(noise_pred, t[0].cpu(), latents).prev_sample\n",
        "\n",
        "    images = self.decode_latents_sampling(latents)\n",
        "    return images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "FCb4jIjk7_bG"
      },
      "outputs": [],
      "source": [
        "vd = DualGuidedVersatileDiffusion()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ee8rN5WI7wi"
      },
      "outputs": [],
      "source": [
        "loss = vd.train_step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "\n",
        "# let's download an initial image\n",
        "url = \"https://huggingface.co/datasets/diffusers/images/resolve/main/benz.jpg\"\n",
        "\n",
        "response = requests.get(url)\n",
        "image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "text = \"a double decker bus\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "images = vd.sampling(image, text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(image[0])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1b421c949ff04907ba5e373217c1db67": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "48b87d5efb9f4d3595e6caf620e7b848": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ac9ba4e47a64987acc993b23fb75a97": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "712726b951074a5aabc64854b3500801": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a46b5f271044fe9b4443987caddce9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ac9ba4e47a64987acc993b23fb75a97",
            "placeholder": "​",
            "style": "IPY_MODEL_d4715644642c41b1a40222bd4017202a",
            "value": " 1000/1000 [01:54&lt;00:00,  6.64it/s]"
          }
        },
        "9793834a26e84a6c962950bcfd7b1a40": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48b87d5efb9f4d3595e6caf620e7b848",
            "placeholder": "​",
            "style": "IPY_MODEL_1b421c949ff04907ba5e373217c1db67",
            "value": "100%"
          }
        },
        "ccf49cc5e1264ea59ef00ec5499e24d0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4715644642c41b1a40222bd4017202a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6de3fe39ecb408a894545afa011c90e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9793834a26e84a6c962950bcfd7b1a40",
              "IPY_MODEL_e33eb6faff664354ad569ba049711fcd",
              "IPY_MODEL_7a46b5f271044fe9b4443987caddce9e"
            ],
            "layout": "IPY_MODEL_712726b951074a5aabc64854b3500801"
          }
        },
        "e33eb6faff664354ad569ba049711fcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ccf49cc5e1264ea59ef00ec5499e24d0",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f54077a8631845519effd54feefeaac9",
            "value": 1000
          }
        },
        "f54077a8631845519effd54feefeaac9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
